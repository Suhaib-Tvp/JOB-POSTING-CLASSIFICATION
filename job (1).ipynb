{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlytI1dsPlwJ",
        "outputId": "e1f15e10-954e-46c1-988a-afb989405f1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for: data science\n",
            "Scraping page 1 for keyword 'data science'\n",
            "Scraping page 2 for keyword 'data science'\n",
            "Scraping page 3 for keyword 'data science'\n",
            "Scraping page 4 for keyword 'data science'\n",
            "Scraping page 5 for keyword 'data science'\n",
            "Searching for: data analyst\n",
            "Scraping page 1 for keyword 'data analyst'\n",
            "Scraping page 2 for keyword 'data analyst'\n",
            "Scraping page 3 for keyword 'data analyst'\n",
            "Scraping page 4 for keyword 'data analyst'\n",
            "Scraping page 5 for keyword 'data analyst'\n",
            "Searching for: data scientist\n",
            "Scraping page 1 for keyword 'data scientist'\n",
            "Scraping page 2 for keyword 'data scientist'\n",
            "Scraping page 3 for keyword 'data scientist'\n",
            "Scraping page 4 for keyword 'data scientist'\n",
            "Scraping page 5 for keyword 'data scientist'\n",
            "Searching for: software engineer\n",
            "Scraping page 1 for keyword 'software engineer'\n",
            "Scraping page 2 for keyword 'software engineer'\n",
            "Scraping page 3 for keyword 'software engineer'\n",
            "Scraping page 4 for keyword 'software engineer'\n",
            "Scraping page 5 for keyword 'software engineer'\n",
            "        Keyword                                              Title  \\\n",
            "0  data science   Principal Product Manager - Growth, Poe (Remote)   \n",
            "1  data science          Machine Learning Physical Design Engineer   \n",
            "2  data science  Staff Software Engineer - Monetization, Poe (R...   \n",
            "3  data science  Staff Backend Engineer - Bot Creator Ecosystem...   \n",
            "4  data science  Senior Backend Engineer - Bot Creator Ecosyste...   \n",
            "\n",
            "       Company                     Location Experience  \\\n",
            "0  Quora, Inc.                        India   6-8 year   \n",
            "1       Google  Bengaluru, Karnataka, India   4-6 year   \n",
            "2  Quora, Inc.                        India  8-10 year   \n",
            "3  Quora, Inc.                        India  8-10 year   \n",
            "4  Quora, Inc.                        India   6-8 year   \n",
            "\n",
            "                                             Summary  \\\n",
            "0  About Quora:Quora’s mission is to grow and sha...   \n",
            "1  Minimum qualifications:Bachelor's degree in El...   \n",
            "2  About Quora:Quora’s mission is to grow and sha...   \n",
            "3  About Quora:Quora’s mission is to grow and sha...   \n",
            "4  About Quora:Quora’s mission is to grow and sha...   \n",
            "\n",
            "                                              Skills  \n",
            "0  Aartificial intelligence,Data Analytics,Data s...  \n",
            "1  Aartificial intelligence,Algorithms,Data struc...  \n",
            "2  Aartificial intelligence,Analytical and Proble...  \n",
            "3  Aartificial intelligence,API,Data science tech...  \n",
            "4  Aartificial intelligence,API,Data science tech...  \n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def scrape_karkidi_jobs(keywords=[\"data science\", \"data analyst\", \"data scientist\", \"software engineer\"], pages=1):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    base_url = \"https://www.karkidi.com/Find-Jobs/{page}/all/India?search={query}\"\n",
        "    jobs_list = []\n",
        "\n",
        "    for keyword in keywords:\n",
        "        print(f\"Searching for: {keyword}\")\n",
        "        for page in range(1, pages + 1):\n",
        "            url = base_url.format(page=page, query=keyword.replace(' ', '%20'))\n",
        "            print(f\"Scraping page {page} for keyword '{keyword}'\")\n",
        "            response = requests.get(url, headers=headers)\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "            job_blocks = soup.find_all(\"div\", class_=\"ads-details\")\n",
        "            for job in job_blocks:\n",
        "                try:\n",
        "                    title = job.find(\"h4\").get_text(strip=True)\n",
        "                    company = job.find(\"a\", href=lambda x: x and \"Employer-Profile\" in x).get_text(strip=True)\n",
        "                    location = job.find(\"p\").get_text(strip=True)\n",
        "                    experience = job.find(\"p\", class_=\"emp-exp\").get_text(strip=True)\n",
        "                    key_skills_tag = job.find(\"span\", string=\"Key Skills\")\n",
        "                    skills = key_skills_tag.find_next(\"p\").get_text(strip=True) if key_skills_tag else \"\"\n",
        "                    summary_tag = job.find(\"span\", string=\"Summary\")\n",
        "                    summary = summary_tag.find_next(\"p\").get_text(strip=True) if summary_tag else \"\"\n",
        "\n",
        "                    jobs_list.append({\n",
        "                        \"Keyword\": keyword,\n",
        "                        \"Title\": title,\n",
        "                        \"Company\": company,\n",
        "                        \"Location\": location,\n",
        "                        \"Experience\": experience,\n",
        "                        \"Summary\": summary,\n",
        "                        \"Skills\": skills\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                    print(f\"Error parsing job block: {e}\")\n",
        "                    continue\n",
        "\n",
        "            time.sleep(1)\n",
        "\n",
        "    return pd.DataFrame(jobs_list)\n",
        "\n",
        "# Usage\n",
        "if __name__ == \"__main__\":\n",
        "    df_jobs = scrape_karkidi_jobs(pages=5)\n",
        "    print(df_jobs.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "\n",
        "def determine_optimal_clusters(skill_matrix, max_k=10):\n",
        "    silhouette_scores = []\n",
        "    k_range = range(2, max_k + 1)\n",
        "\n",
        "    for k in k_range:\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "        labels = kmeans.fit_predict(skill_matrix)\n",
        "        score = silhouette_score(skill_matrix, labels)\n",
        "        silhouette_scores.append(score)\n",
        "\n",
        "\n",
        "\n",
        "    # Return the best k based on highest silhouette score\n",
        "    best_k = k_range[silhouette_scores.index(max(silhouette_scores))]\n",
        "    return best_k\n",
        "\n",
        "def assign_clusters_to_jobs(job_dataframe):\n",
        "    # Convert skills text into TF-IDF features\n",
        "    tfidf = TfidfVectorizer(stop_words='english')\n",
        "    skill_matrix = tfidf.fit_transform(job_dataframe['Skills'])\n",
        "\n",
        "    # Determine the optimal number of clusters\n",
        "    optimal_k = determine_optimal_clusters(skill_matrix, max_k=10)\n",
        "\n",
        "    # Perform KMeans clustering using the optimal number of clusters\n",
        "    clustering_model = KMeans(n_clusters=optimal_k, random_state=42)\n",
        "    labels = clustering_model.fit_predict(skill_matrix)\n",
        "\n",
        "    # Add cluster labels to the DataFrame\n",
        "    job_dataframe['Cluster'] = labels\n",
        "\n",
        "    return job_dataframe, clustering_model, tfidf\n",
        "\n",
        "# Example usage:\n",
        "clustered_jobs, model_used, tfidf_used = assign_clusters_to_jobs(df_jobs)\n",
        "print(clustered_jobs.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzrCS6FSWeLM",
        "outputId": "b5dc6910-72d7-47fa-db21-6e824eaa888b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Keyword                                              Title  \\\n",
            "0  data science   Principal Product Manager - Growth, Poe (Remote)   \n",
            "1  data science          Machine Learning Physical Design Engineer   \n",
            "2  data science  Staff Software Engineer - Monetization, Poe (R...   \n",
            "3  data science  Staff Backend Engineer - Bot Creator Ecosystem...   \n",
            "4  data science  Senior Backend Engineer - Bot Creator Ecosyste...   \n",
            "\n",
            "       Company                     Location Experience  \\\n",
            "0  Quora, Inc.                        India   6-8 year   \n",
            "1       Google  Bengaluru, Karnataka, India   4-6 year   \n",
            "2  Quora, Inc.                        India  8-10 year   \n",
            "3  Quora, Inc.                        India  8-10 year   \n",
            "4  Quora, Inc.                        India   6-8 year   \n",
            "\n",
            "                                             Summary  \\\n",
            "0  About Quora:Quora’s mission is to grow and sha...   \n",
            "1  Minimum qualifications:Bachelor's degree in El...   \n",
            "2  About Quora:Quora’s mission is to grow and sha...   \n",
            "3  About Quora:Quora’s mission is to grow and sha...   \n",
            "4  About Quora:Quora’s mission is to grow and sha...   \n",
            "\n",
            "                                              Skills  Cluster  \n",
            "0  Aartificial intelligence,Data Analytics,Data s...        6  \n",
            "1  Aartificial intelligence,Algorithms,Data struc...        2  \n",
            "2  Aartificial intelligence,Analytical and Proble...        7  \n",
            "3  Aartificial intelligence,API,Data science tech...        0  \n",
            "4  Aartificial intelligence,API,Data science tech...        0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:1389: ConvergenceWarning: Number of distinct clusters (9) found smaller than n_clusters (10). Possibly due to duplicate points in X.\n",
            "  return fit_method(estimator, *args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "def persist_models(model, tfidf_vectorizer, model_path='kmeans_model.pkl', vectorizer_path='vectorizer.pkl'):\n",
        "\n",
        "    joblib.dump(model, model_path)\n",
        "    joblib.dump(tfidf_vectorizer, vectorizer_path)\n",
        "\n",
        "# Example usage (use after fitting your model):\n",
        "# Use the variables assigned in the previous cell\n",
        "persist_models(model_used, tfidf_used)"
      ],
      "metadata": {
        "id": "l5hyAqOaW9q5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def predict_job_cluster(skills_text, model, tfidf_vec):\n",
        "    # Convert the input skills text into vector form using the TF-IDF vectorizer\n",
        "    features = tfidf_vec.transform([skills_text])\n",
        "    predicted_label = model.predict(features)\n",
        "    return predicted_label[0]\n",
        "\n",
        "# Load the saved model and vectorizer\n",
        "model = joblib.load('kmeans_model.pkl')\n",
        "vectorizer = joblib.load('vectorizer.pkl')\n",
        "\n",
        "# Predict cluster for sample input\n",
        "sample_skills = \"AWS, Python, Data Science, Machine Learning\"\n",
        "cluster = predict_job_cluster(sample_skills, model, vectorizer)\n",
        "print(f\"Predicted cluster: {cluster}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J81W6G6XWWX",
        "outputId": "b5accdc9-8263-4062-d909-f9bc4217a913"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted cluster: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install schedule\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzkZhdRrX4EW",
        "outputId": "83611397-9881-4775-88ad-48075d85e7f8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting schedule\n",
            "  Downloading schedule-1.2.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Downloading schedule-1.2.2-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: schedule\n",
            "Successfully installed schedule-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import schedule\n",
        "import smtplib\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.text import MIMEText\n",
        "import time\n",
        "import joblib\n",
        "\n",
        "# Load persisted model and vectorizer\n",
        "model = joblib.load('kmeans_model.pkl')\n",
        "vectorizer = joblib.load('vectorizer.pkl')\n",
        "\n",
        "def email_alert(subject_line, message, recipient):\n",
        "    sender = \"your_email@example.com\"\n",
        "    sender_password = \"your_password\"\n",
        "    smtp_server = smtplib.SMTP('smtp.example.com', 587)\n",
        "    smtp_server.starttls()\n",
        "    smtp_server.login(sender, sender_password)\n",
        "\n",
        "    email_message = MIMEMultipart()\n",
        "    email_message['From'] = sender\n",
        "    email_message['To'] = recipient\n",
        "    email_message['Subject'] = subject_line\n",
        "    email_message.attach(MIMEText(message, 'plain'))\n",
        "\n",
        "    smtp_server.sendmail(sender, recipient, email_message.as_string())\n",
        "    smtp_server.quit()\n",
        "\n",
        "def scrape_and_notify_daily():\n",
        "    jobs_df = scrape_karkidi_jobs(keyword=\"data science\", pages=2)\n",
        "\n",
        "    # Use existing model and vectorizer to assign clusters\n",
        "    skill_matrix = vectorizer.transform(jobs_df['Skills'])\n",
        "    labels = model.predict(skill_matrix)\n",
        "    jobs_df['Cluster'] = labels\n",
        "\n",
        "    # Find jobs matching user interests\n",
        "    interests = [\"data science\", \"python\",\"software engineer\", \"data analyst\"]\n",
        "    matched_jobs = jobs_df[jobs_df['Skills'].str.contains('|'.join(interests), case=False)]\n",
        "\n",
        "    if not matched_jobs.empty:\n",
        "        subj = \"Job Updates Based on Your Interests\"\n",
        "        body_text = matched_jobs.to_string(index=False)\n",
        "        email_alert(subj, body_text, \"user@example.com\")\n",
        "\n",
        "# Schedule the daily task at 10 AM\n",
        "schedule.every().day.at(\"10:00\").do(scrape_and_notify_daily)\n",
        "\n",
        "while True:\n",
        "    schedule.run_pending()\n",
        "    time.sleep(1)\n"
      ],
      "metadata": {
        "id": "Nm8SGY00Xgtu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}